{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2bb426",
   "metadata": {},
   "source": [
    "# 1. write a function which will try to find out len of a string without using an inbuilt len function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e292c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def length_of_string(s):\n",
    "    \"\"\"This functions returns the length of a string\"\"\"\n",
    "    count=0\n",
    "    for i in s:\n",
    "        count=count+1\n",
    "    return count\n",
    "s=\"ABCDEFGHIJKLMN\"\n",
    "length_of_string(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fd89a",
   "metadata": {},
   "source": [
    "# 2. write a function which  will be able to print an index of list element without using an index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a2e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 45: 2\n",
      "Index of 243: The element is not present in the list\n"
     ]
    }
   ],
   "source": [
    "def index_ele(l,ele):\n",
    "    \"\"\"This function takes a list and an element as arguments and returns the index of the element in the list\"\"\"\n",
    "    f=0\n",
    "    for i in range(len(l)):\n",
    "        if l[i]==ele:\n",
    "            index_value=i\n",
    "            f=1\n",
    "    if f==1:\n",
    "        return index_value\n",
    "    else:\n",
    "        return \"The element is not present in the list\"\n",
    "l=[1254,67,45,23,24,89,12,22]\n",
    "print(f\"Index of {45}: {index_ele(l,45)}\")\n",
    "print(f\"Index of {243}: {index_ele(l,243)}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4e5b8",
   "metadata": {},
   "source": [
    "# 3. write a function which will be able to print an ip address of your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402c8a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP Address of the system: 192.168.29.172\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def ip_address_of_system():\n",
    "    host=socket.gethostname()\n",
    "    ip_address=socket.gethostbyname(host)\n",
    "    print(f\"IP Address of the system: {ip_address}\")\n",
    "ip_address_of_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd5b9a",
   "metadata": {},
   "source": [
    "# 4. write a function which will shutdown your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e797b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def shutdown_system():\n",
    "    os.system(\"shutdown /s /t 1\")\n",
    "shutdown_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40221fba",
   "metadata": {},
   "source": [
    "# 5. write a function which will take input as a list with any kind of numeric value and give an out as a multiplication of all the numeric data l = [3.5, 6.56, 4,5,\"sudh\" , \"ineuron\" , 'fsda bootcamp 2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "add4fd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459.19999999999993"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply_list(l):\n",
    "    s=1\n",
    "    for i in l:\n",
    "        if type(i)==int or type(i)==float:\n",
    "            s=s*i\n",
    "    return s\n",
    "l = [3.5, 6.56, 4,5,\"sudh\" , \"ineuron\" , 'fsda bootcamp 2.0']\n",
    "multiply_list(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245069b",
   "metadata": {},
   "source": [
    "# 6. write a function which will be able to read all the mails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c25e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b2c3ba4",
   "metadata": {},
   "source": [
    "# 7 . write a function which will be able to send a mail to anyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0980571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49dba754",
   "metadata": {},
   "source": [
    "# 8. write a function which will be able to read a doc/word file from your system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b1dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVELOPMENT OF AUTONOMOUS CARS USING DEEP    LEARNING\n",
      "\n",
      "\n",
      "\n",
      " Team Members: \n",
      "\n",
      "Varush H - 17BEE0154 \n",
      "\n",
      "Remala Jaya Sabarish Reddy - 17BEE0274 Rahul Srinivas - 17BEE0136 \n",
      "\n",
      " \n",
      "\n",
      "Under the Guidance of \n",
      "\n",
      "                                Dr. P Arulmozhivarman Professor \n",
      "\n",
      " \n",
      "\n",
      "School of Electrical Engineering \n",
      "\n",
      "VIT, Vellore \n",
      "\n",
      "TARP Project EEE3999 \n",
      "\n",
      "Fall Semester 2020-21 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents: \n",
      "\n",
      " \n",
      "\n",
      "Introduction \n",
      "\n",
      "Problem Identification/ Motivation \n",
      "\n",
      "Objectives \n",
      "\n",
      "Literature Review-survey \n",
      "\n",
      "Detailed Methodology  \n",
      "\n",
      "Experimental Design/Data Collection/Survey \n",
      "\n",
      "Design Analysis/Data Cleaning/Survey analysis \n",
      "\n",
      "Algorithm Development \n",
      "\n",
      "Trade off Identified \n",
      "\n",
      "Algorithm Implementation \n",
      "\n",
      "Demonstration of Outcome/ Results and Discussion \n",
      "\n",
      "Problems faced or Challenges \n",
      "\n",
      "Individual Contribution \n",
      "\n",
      "Milestone and time frame  \n",
      "\n",
      "References \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Annex: \n",
      "\n",
      "Code/ Link \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTRODUCTION: \n",
      "\n",
      "An Autonomous Car, also known as a self-driving car, driverless car, or a robotic car, is a vehicle that is capable of sensing its environment and moving safely with little or no human input. Autonomous cars are poised to revolutionize the transportation industry. Newer cars already have automated features for things like parking and collision detection. Tech companies now are hard at work to deliver vehicles that are capable of advanced navigation without input from a human driver either full controlled or automated. So, we took up this challenge of making a Driverless car using advanced computer techniques such as Behaviour Cloning and Deep Learning. Autonomous cars can solve many problems, like traffic delays, accidents and traffic collisions caused by driver error. In our project they are typically driven around and trained on real roads by manual drivers and they are then trained on the data that they collected on this drive to then clone the behaviour of their manual drivers. We are using Complex Deep learning Techniques and Image Manipulation. \n",
      "\n",
      "PROBLEM IDENTIFICATION / MOTIVATION: \n",
      "\n",
      "Driverless cars stand to solve all sorts of problems, like traffic delays and traffic collisions caused by driver error. \n",
      "\n",
      "Autonomous vehicles will bring to market all sorts of new and exciting applications for a variety of industries, like shipping, transportation, and emergency transportation. \n",
      "\n",
      "The ultimate goal of self-driving cars is to delegate the responsibility of driving to a machine. \n",
      "\n",
      "Here, we use a neural network to clone car driving behavior. It is a supervised regression problem between the car steering angles and the road images in front of a car.  \n",
      "\n",
      "OBJECTIVES: \n",
      "\n",
      "To train an end-to-end deep learning model that would let a car drive by itself around the track in a driving simulator.  \n",
      "\n",
      "We’ll use Udacity’s driving simulator which has two different tracks. One of them will used for collecting training data, and the other one — never seen by the model — as a substitute for the test set. \n",
      "\n",
      "For training our model, we would drive the car using arrow keys in the training track and we collect the data by recording in the simulator.  \n",
      "\n",
      "Now after we train our model, we would evaluate its performance on a completely different ‘testing track’ where the car will be made to run autonomously.  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "LITERATURE SURVEY: \n",
      "\n",
      "S No. \n",
      "\n",
      "Name of the paper \n",
      "\n",
      "Authors \n",
      "\n",
      "Abstract and objectives of the paper \n",
      "\n",
      "1 \n",
      "\n",
      "Convolutional Neural \n",
      "\n",
      "Network for a \n",
      "\n",
      "Self-Driving \n",
      "\n",
      "Car in a \n",
      "\n",
      "Virtual \n",
      "\n",
      "Environment \n",
      "\n",
      "M. A. A. Babiker, \n",
      "\n",
      "M. A. O. Elawad and A. H. M. Ahmed \n",
      "\n",
      "For a long time, traditional computer vision-based \n",
      "\n",
      "algorithms have been the \n",
      "\n",
      "primary methods for analysing camera footage, used for \n",
      "\n",
      "assisting safety functions, \n",
      "\n",
      "where decision making have been a product of manually \n",
      "\n",
      "constructed behaviours. During \n",
      "\n",
      "the last few years deep learning \n",
      "\n",
      "has showed its extraordinary \n",
      "\n",
      "capabilities for both visual recognition and decision \n",
      "\n",
      "making in end-to-end systems. this paper proposes a solution \n",
      "\n",
      "of introducing redundancy by combining deep learning methods with traditional computer vision-based \n",
      "\n",
      "techniques for minimizing unsafe behaviour in \n",
      "\n",
      "autonomous vehicles. A CNN has been trained to map raw \n",
      "\n",
      "pixels from a single front-facing camera directly to steering \n",
      "\n",
      "commands. The objective was to build a simple and reliable \n",
      "\n",
      "algorithm for a self-driving car \n",
      "\n",
      "and to implement a system that allow autonomous driving. \n",
      "\n",
      "2 \n",
      "\n",
      "Towards selfdriving car using convolutional neural network and \n",
      "\n",
      "B. T. \n",
      "\n",
      "Nugraha, \n",
      "\n",
      "S. Su and \n",
      "\n",
      "Fahmizal \n",
      "\n",
      "The advancement of Computer Vision these days has grown up beyond imagination. Recently, many researchers and tech companies are competing to develop self-driving car using \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "road lane detector \n",
      "\n",
      "\n",
      "\n",
      "either traditional or deep learning approach. YOLO (You Only Look Once) is one of the real-time CNN methods that aims to detect objects from images. On the other hand, Road Lane Detector is used to detect road track from video's frames and to provide additional information that can be helpful for the decisionmaking process of the selfdriving car. In this paper, we use YOLO as the object detector and polynomial regression as the road guidance in the real-world driving video simulations. We use NVIDIA GTX 1070 with 8 GB of RAM for the computations. The result shows a matching pair between those two methods for selfdriving car environment and road lane guidance. \n",
      "\n",
      "3. \n",
      "\n",
      "Highly \n",
      "\n",
      "Automated \n",
      "\n",
      "Vehicles and \n",
      "\n",
      "Self-Driving \n",
      "\n",
      "Cars [Industry \n",
      "\n",
      "Tutorial] \n",
      "\n",
      "Á. Takács, \n",
      "\n",
      "I. Rudas, D. Bösl and T. Haidegger \n",
      "\n",
      "Self-driving cars have, in recent years, clearly become among the most actively discussed and researched topics. By all definitions, these systems, as a third robotic revolution, belong to the robotics field, despite the fact that people generally assign them to a specific domain of the automotive industry. Replicating the complex task of human driving by an autonomous system poses countless engineering challenges, involving the wider field of robotics, including \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "environment perception, decision making, and control. \n",
      "\n",
      "4 \n",
      "\n",
      "Autonomous \n",
      "\n",
      "Vehicles: \n",
      "\n",
      "Developing a public health research agenda to frame the future transportation policy \n",
      "\n",
      "Travis J Crayton and Benjamin \n",
      "\n",
      "Mason \n",
      "\n",
      "Meier \n",
      "\n",
      "Recent advancements in autonomous \n",
      "\n",
      "vehicle technology have led to projections that fully autonomous vehicles could define the transportation network within the coming years. In preparation for this disruptive innovation in transportation technology, transportation scholars have started to assess the potential impacts of autonomous vehicles, and transportation policymakers have started to formulate policy recommendations and regulatory guidance concerning their deployment. However, there has been little analysis of the public health implications arising from the widespread adoption of fully autonomous vehicles. We examine these prospective public health impacts—both benefits and harms to individual and population health—and analyze how they can be considered in the development of transportation policy. In this manuscript, we discuss the evolving relationship between technological innovations in transportation and public health, conceptualize automated transportation as a disruptive technology \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "necessitating a public policy response, and define a research agenda to examine the public health implications of autonomous vehicle policy, as seen through existing evidence on road casualties, environmental health, aging populations, noncommunicable disease, land use, and labor markets.  \n",
      "\n",
      " \n",
      "\n",
      "DETAILED METHODOLOGY: \n",
      "\n",
      "As we drive the car through the simulator, we are going to be taking images at each instance of the drive. These images are going to represent our training data set and the label for each specific image is going to be the steering angle of the car at that specific instance.  \n",
      "\n",
      "We will then show all of these images to our Convolutional Neural Network (CNN) and allow it to learn how to drive autonomously by learning from our behavior as a manual driver.  \n",
      "\n",
      "The driving simulator would save frames from three front-facing “cameras”, recording data from the car’s point of view; as well as various driving statistics like the throttle, speed, and steering angle. We are going to use camera data as model input and expect it to predict the steering angle in the [-1, 1] range. \n",
      "\n",
      "Now after we train our model, we are going to evaluate its performance on a completely different ‘testing track’ where the car will be made to run autonomously. If we can train the car properly, it will perform very well on our second track and will drive on its own.  \n",
      "\n",
      "The Behavioral Cloning Technique is incredibly useful and plays a big role in real-life Self Driving Cars as well. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FLOW CHART: \n",
      "\n",
      " \n",
      "\n",
      "DATA COLLECTION: \n",
      "\n",
      "We used Udacity’s simulator to collect our data and python programming language here to develop our model and do analysis. \n",
      "\n",
      "In this driving simulator there are two types of modes and two tracks. One is Training mode and other is Autonomous mode. \n",
      "\n",
      "Firstly, we have to drive the car in the Training track and we should collect the data by recording in the simulator. Normal Arrow Keys are used to drive the car. Once the recording is done we can see our data in the folder that we selected before recording. \n",
      "\n",
      "An Excel sheet of Data and Image folder is produced. The excel sheet contains lots of data aligned in the coloumns. A sample of it is shown below. \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "DATA ANALYSIS: \n",
      "\n",
      "    Steps Performed: \n",
      "\n",
      "Importing Libraries \n",
      "\n",
      "Loading Dataset  \n",
      "\n",
      "Data Exploaration \n",
      "\n",
      "Visualizing the dependent variable \n",
      "\n",
      "Data Cleaning \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We will then split the images into training and validation set in order to measure the performance at every epoch.  \n",
      "\n",
      "We split the data using the 80–20 rule which means using 80% of the data for training while the rest for testing the model on unseen images. \n",
      "\n",
      " \n",
      "\n",
      "Image Augmentation: \n",
      "\n",
      "For training, we use the following augmentation technique along with Python generator to generate unlimited number of images: \n",
      "\n",
      "Randomly choose right, left or center images.  \n",
      "\n",
      "For left image, steering angle is adjusted by +0.2  \n",
      "\n",
      "For right image, steering angle is adjusted by -0.2  \n",
      "\n",
      "Randomly flip image left/right  \n",
      "\n",
      "Randomly translate image horizontally with steering angle adjustment (0.002 per pixel shift)  \n",
      "\n",
      "Randomly translate image vertically  \n",
      "\n",
      "Randomly added shadows \n",
      "\n",
      "Randomly altering image brightness (lighter or darker)  \n",
      "\n",
      "Using the left/right images is useful to train the recovery driving scenario. The horizontal translation is useful for difficult curve handling (i.e. the one after the bridge).  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "Image pre-processing: \n",
      "\n",
      "Before we implement the deep learning algorithm, we need to do image pre-processing. This is done to remove some of the unnecessary things like the scenery, trees etc from the image.  \n",
      "\n",
      "Firstly we reduce the size of the image. Then using computer vision, we convert the image from RGB format to YUV format. \n",
      "\n",
      "After this, we smoothen the image using GaussianBlur. \n",
      "\n",
      "Then, we resize the image. \n",
      "\n",
      "Finally, we normalize the image. The normalized image is the image we take as input in the deep learning algorithm. \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FINDING LANES: \n",
      "\n",
      "Import image to cv2 library and convert the pixels in to matrix \n",
      "\n",
      "Convert image to grayscale. \n",
      "\n",
      "Access of matrix values as pixels and show. \n",
      "\n",
      "reduce the noise. \n",
      "\n",
      "Edge Detection  \n",
      "\n",
      "Selecting the single lane from mask and canny \n",
      "\n",
      "Display picture for infinite time. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ALGORITHM DEVELOPMENT (DEEP LEARNING ALGORITHM): \n",
      "\n",
      "NVIDIA Architecture : \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "  ALGORITHM IMPLEMENTATION/ PARAMETERS: \n",
      "\n",
      "Activation function : Exponential Linear Unit (elu) \n",
      "\n",
      "Learning Rate: 1*e-3 \n",
      "\n",
      "Method Adopted to calculate loss= Mean Square Error Method \n",
      "\n",
      "Epochs= 10 \n",
      "\n",
      "Time taken to train model for each epoch is ~444seconds \n",
      "\n",
      "Image input size = 200*66 \n",
      "\n",
      "300 training steps per epoch (consisting of 100 images per epoch) \n",
      "\n",
      "200 validating steps per epoch  (consisting of 100 images per epoch) \n",
      "\n",
      "\n",
      "\n",
      "DEMONSTRATION OF RESULTS:\n",
      "\n",
      "After we train our model, we have evaluated its performance on a completely different ‘testing track’ where the car is made to run autonomously. \n",
      "\n",
      "The Behavioral Cloning Technique is incredibly useful and plays a big role in real-life Self Driving Cars as well. \n",
      "\n",
      "We have trained our model perfectly, by testing it in new track (autonomous mode) shown below our car will move without hitting the edge of the road. Then we came to know that we have trained our model perfectly.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/30 \n",
      "\n",
      "300/300 [==============================] - 455s 2s/step - loss: 0.0875 - val_loss: 0.0655 Epoch 2/30 \n",
      "\n",
      "300/300 [==============================] - 453s 2s/step - loss: 0.0862 - val_loss: 0.0698 Epoch 3/30 300/300 [==============================] - 454s 2s/step - loss: 0.0844 - val_loss: 0.0662 \n",
      "\n",
      "Epoch 4/30 300/300 [==============================] - 445s 1s/step - loss: 0.0832 - val_loss: 0.0602 \n",
      "\n",
      "Epoch 5/30 300/300 [==============================] - 448s 1s/step - loss: 0.0834 - val_loss: 0.0663 \n",
      "\n",
      "Epoch 6/30 300/300 [==============================] - 447s 1s/step - loss: 0.0785 - val_loss: 0.0552 Epoch 7/30 \n",
      "\n",
      "300/300 [==============================] - 450s 1s/step - loss: 0.0797 - val_loss: 0.0582 Epoch 8/30 \n",
      "\n",
      "300/300 [==============================] - 447s 1s/step - loss: 0.0765 - val_loss: 0.0546 Epoch 9/30 300/300 [==============================] - 453s 2s/step - loss: 0.0763 - val_loss: 0.0525 \n",
      "\n",
      "Epoch 10/30 300/300 [==============================] - 450s 2s/step - loss: 0.0751 - val_loss: 0.0553 \n",
      "\n",
      "Epoch 11/30 300/300 [==============================] - 450s 2s/step - loss: 0.0737 - val_loss: 0.0509 \n",
      "\n",
      "Epoch 12/30 300/300 [==============================] - 453s 2s/step - loss: 0.0726 - val_loss: 0.0526 Epoch 13/30 \n",
      "\n",
      "300/300 [==============================] - 453s 2s/step - loss: 0.0698 - val_loss: 0.0472 Epoch 14/30 \n",
      "\n",
      "300/300 [==============================] - 451s 2s/step - loss: 0.0714 - val_loss: 0.0547 Epoch 15/30 \n",
      "\n",
      "300/300 [==============================] - 454s 2s/step - loss: 0.0662 - val_loss: 0.0427 Epoch 16/30 300/300 [==============================] - 459s 2s/step - loss: 0.0661 - val_loss: 0.0459 \n",
      "\n",
      "Epoch 17/30 300/300 [==============================] - 457s 2s/step - loss: 0.0637 - val_loss: 0.0459 \n",
      "\n",
      "Epoch 18/30 300/300 [==============================] - 453s 2s/step - loss: 0.0634 - val_loss: 0.0464 \n",
      "\n",
      "Epoch 19/30 300/300 [==============================] - 454s 2s/step - loss: 0.0624 - val_loss: 0.0430 Epoch 20/30 \n",
      "\n",
      "300/300 [==============================] - 458s 2s/step - loss: 0.0625 - val_loss: 0.0474 Epoch 21/30 \n",
      "\n",
      "300/300 [==============================] - 455s 2s/step - loss: 0.0608 - val_loss: 0.0473 Epoch 22/30 300/300 [==============================] - 450s 2s/step - loss: 0.0590 - val_loss: 0.0361 \n",
      "\n",
      "Epoch 23/30 300/300 [==============================] - 450s 2s/step - loss: 0.0614 - val_loss: 0.0535 \n",
      "\n",
      "Epoch 24/30 300/300 [==============================] - 456s 2s/step - loss: 0.0574 - val_loss: 0.0422 \n",
      "\n",
      "Epoch 25/30 300/300 [==============================] - 457s 2s/step - loss: 0.0568 - val_loss: 0.0463 Epoch 26/30 \n",
      "\n",
      "300/300 [==============================] - 454s 2s/step - loss: 0.0575 - val_loss: 0.0346 Epoch 27/30 \n",
      "\n",
      "300/300 [==============================] - 452s 2s/step - loss: 0.0575 - val_loss: 0.0441 Epoch 28/30 \n",
      "\n",
      "300/300 [==============================] - 455s 2s/step - loss: 0.0571 - val_loss: 0.0546 Epoch 29/30 300/300 [==============================] - 455s 2s/step - loss: 0.0551 - val_loss: 0.0421 \n",
      "\n",
      "Epoch 30/30 300/300 [==============================] - 454s 2s/step - loss: 0.0553 - val_loss: 0.0350 \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Video Link: https://drive.google.com/file/d/1gkvCVv23vR5aJcag9FXGh94RPSK2Fgr7/view?usp=sharing \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROBLEMS FACED: \n",
      "\n",
      "We found difficulty in choosing the simulator. First, we tried using AIR-SIM simulator which was bit difficult to use. Then we came to know about the UDACITY simulator which was user friendly. \n",
      "\n",
      "Edge Detection is difficult in non-sunny days for finding the lane marks. \n",
      "\n",
      "Image pre-processing techniques was difficult to implement and to render. \n",
      "\n",
      "While finding lanes frames per second should be in the nominal order of 25-35 fps.  \n",
      "\n",
      "Running epochs for training model is taking time so as to analyse the errors. (approx. 444sec per epoch) \n",
      "\n",
      "\n",
      "\n",
      "TRADE- OFFS IDENTIFIED: \n",
      "\n",
      "Risk is an important trade-off in Autonomous cars. Humans driving newly developed Autonomous cars is definitely a big risk but in return, it is easier to use it. \n",
      "\n",
      "In our algorithm, we had to reduce the frames per second while finding lanes in order to achieve maximum accuracy. \n",
      "\n",
      "While monitoring the CNN, filtering of pixels using keras should be done in an iterative manner but not with simultaneous values. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INDIVIDUAL CONTRIBUTION: \n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      " \n",
      "\n",
      "Contributio\n",
      "\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Varush\n",
      "\n",
      " \n",
      "\n",
      "Data Collection\n",
      "\n",
      ",\n",
      "\n",
      " \n",
      "\n",
      "Analysis and \n",
      "\n",
      "Image Augmentation\n",
      "\n",
      ".\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Sabarish\n",
      "\n",
      " \n",
      "\n",
      "Finding Lanes and Deep Learning \n",
      "\n",
      "Model\n",
      "\n",
      ".\n",
      "\n",
      " \n",
      "\n",
      "Rahul Srinivas\n",
      "\n",
      " \n",
      "\n",
      "Image pre-processing, Interfacing code and simulator\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Name\n",
      "\n",
      " \n",
      "\n",
      "Contributio\n",
      "\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Varush\n",
      "\n",
      " \n",
      "\n",
      "Data Collection\n",
      "\n",
      ",\n",
      "\n",
      " \n",
      "\n",
      "Analysis and \n",
      "\n",
      "Image Augmentation\n",
      "\n",
      ".\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Sabarish\n",
      "\n",
      " \n",
      "\n",
      "Finding Lanes and Deep Learning \n",
      "\n",
      "Model\n",
      "\n",
      ".\n",
      "\n",
      " \n",
      "\n",
      "Rahul Srinivas\n",
      "\n",
      " \n",
      "\n",
      "Image pre-processing, Interfacing code and simulator\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MILESTONE AND TIME FRAME:\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "REFERENCES: \n",
      "\n",
      "M. A. A. Babiker, M. A. O. Elawad and A. H. M. Ahmed, \n",
      "\n",
      "\"Convolutional Neural Network for a Self-Driving Car in a \n",
      "\n",
      "Virtual Environment,\" 2019 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE), Khartoum, Sudan, 2019, pp. 1-6, doi: \n",
      "\n",
      "10.1109/ICCCEEE46830.2019.9070826. \n",
      "\n",
      "B. T. Nugraha, S. Su and Fahmizal, \"Towards self-driving car using convolutional neural network and road lane detector,\" 2017 2nd International Conference on Automation, Cognitive \n",
      "\n",
      "Science, Optics, Micro Electro-Mechanical System, and Information Technology (ICACOMIT), Jakarta, 2017, pp. 65-69, doi: 10.1109/ICACOMIT.2017.8253388 \n",
      "\n",
      "Á. Takács, I. Rudas, D. Bösl and T. Haidegger, \"Highly Automated Vehicles and Self-Driving Cars [Industry Tutorial],\" in IEEE Robotics & Automation Magazine, vol. 25, no. 4, pp. 106-112, Dec. 2018, doi: 10.1109/MRA.2018.2874301. \n",
      "\n",
      "Q. Rao and J. Frtunikj, \"Deep Learning for Self-Driving Cars: \n",
      "\n",
      "Chances and Challenges,\" 2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS), Gothenburg, 2018, pp. 35-38. \n",
      "\n",
      " \n",
      "\n",
      "Code: \n",
      "\n",
      "https://colab.research.google.com/drive/1JC9FKbTiNDNc-aXIv_BtNbv5GrffvY3U?usp=sharing \n",
      "\n",
      "1 \n",
      "\n",
      " \n",
      "\n",
      "1 \n",
      "\n",
      " \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "def read_doc():\n",
    "    text=textract.process(r\"C:\\Users\\Rahul\\Desktop\\TARP_REPORT.docx\")\n",
    "    text=text.decode(\"utf-8\")\n",
    "    print(text)\n",
    "read_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3216a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
